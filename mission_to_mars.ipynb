{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "from splinter.exceptions import ElementDoesNotExist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "#Url used to scrape data\n",
    "!which chromedriver\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for all information to be saved\n",
    "results_for_mars = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NASA Mars News\n",
    "# Retrieve page with the requests module\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "response=browser.visit(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA's InSight Places First Instrument on Mars\n",
      "In deploying its first instrument onto the surface of Mars, the lander completes a major mission milestone.\n"
     ]
    }
   ],
   "source": [
    "# Examine the results, then determine the element that contains your information\n",
    "news_title = soup.find('div', class_='content_title').text\n",
    "news_p = soup.find('div', class_='rollover_description_inner').text\n",
    "clean_title=news_title.strip()\n",
    "clean_p=news_p.strip()\n",
    "# Run only if title, paragraph are available\n",
    "if (news_title and news_p):\n",
    "    # Print results\n",
    "    print(clean_title)\n",
    "    print(clean_p)\n",
    "\n",
    "    Mars_News = {\n",
    "    'title': clean_title,\n",
    "    'Teaser': clean_p\n",
    "    }\n",
    "    results_for_mars[\"Mars_News\"]= Mars_News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JPL Mars Space Images - Featured Image\n",
    "url_2 = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML object\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_2 = BeautifulSoup(html, 'html.parser')\n",
    "# Retrieve all elements that contain book information\n",
    "image = soup_2.find('div', class_=\"carousel_items\")\n",
    "\n",
    "# Clean to get the Url\n",
    "dirty_url = image.find('article')['style']\n",
    "front_num=dirty_url.find(\"('\")\n",
    "back_num=dirty_url.find(\"')\")\n",
    "par_image=dirty_url[front_num+len(\"('\"):back_num]\n",
    "\n",
    "#Clean the orginal URL\n",
    "back_num_2=url_2.find(\"/s\")\n",
    "url_2_short=url_2[:back_num_2]\n",
    "\n",
    "#Combine the URL for Wallpaper image\n",
    "featured_image_url=url_2_short + par_image\n",
    "results_for_mars[\"featured_image_url\"]= featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### Mars Weather\n",
    "\n",
    "#Find the latest weather by NASA's Mars twitter account\n",
    "url_tweet = 'https://twitter.com/marswxreport?lang=en'\n",
    "response_2 = requests.get(url_tweet)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html'\n",
    "soup_3 = BeautifulSoup(response_2.text, 'html.parser')\n",
    "mars_weather = soup_3.find('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text\n",
    "results_for_mars[\"mars_weather\"]= mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find fun facts\n",
    "# URL\n",
    "url_facts = 'http://space-facts.com/mars/'\n",
    "\n",
    "# Pull in HTML table 1\n",
    "tables = pd.read_html(url_facts)\n",
    "df = tables[0]\n",
    "df.columns = ['Category', 'Units']\n",
    "df_html = df.to_html()\n",
    "df_html_clean= df_html.replace('\\n', '')\n",
    "results_for_mars[\"fun_facts\"]= df_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://web.archive.org/web/20181114171728/https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced\n",
      "Cerberus Hemisphere Enhanced\n",
      "---------\n",
      "http://web.archive.org/web/20181114171728/https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced\n",
      "Schiaparelli Hemisphere Enhanced\n",
      "---------\n",
      "http://web.archive.org/web/20181114171728/https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced\n",
      "Syrtis Major Hemisphere Enhanced\n",
      "---------\n",
      "http://web.archive.org/web/20181114171728/https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced\n",
      "Valles Marineris Hemisphere Enhanced\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Mars Hemispheres Scrape\n",
    "url_hem ='http://web.archive.org/web/20181114171728/https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(url_hem)\n",
    "html = browser.html\n",
    "soup_4 = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "sidebar = soup_4.find_all('div', class_='description')\n",
    "# Clean and retreive the Url\n",
    "hemisphere_image_urls = []\n",
    "work_around_main = 'http://web.archive.org'\n",
    "for article in sidebar:\n",
    "    # Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "    urls= article.find('a')['href']\n",
    "    urls_full= work_around_main+urls\n",
    "    title=article.find('h3').text\n",
    "    print(urls_full)\n",
    "    print(title)\n",
    "    print('---------')\n",
    "    browser.visit(urls_full)\n",
    "    html = browser.html\n",
    "    soups_in_soups = BeautifulSoup(html, 'html.parser')\n",
    "    #     print(soups_in_soups.prettify())\n",
    "    downloads = soups_in_soups.find('div', class_='downloads')\n",
    "    img_url = downloads.find('a')['href']\n",
    "    hemisphere_image_urls.append({\"title\" : title, \"img_url\" : img_url})\n",
    "results_for_mars[\"hemisphere_image_urls\"]= hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
